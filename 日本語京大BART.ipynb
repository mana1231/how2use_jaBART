{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conda作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda create -n [name] python=3.8\n",
    "# ! conda install jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xenhan\n",
    "! pip install zenhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyknp\n",
    "! pip install pyknp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　sentencepiece\n",
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) tensorboardX\n",
    "! pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairseq\n",
    "! git clone https://github.com/utanaka2000/fairseq\n",
    "! pip install --editable fairseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/utanaka2000/fairseq/blob/japanese_bart_pretrained_model/jaBART_preprocess.py \n",
    "\n",
    "↑ここからjaBART_preprocess.py持ってくる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jumanpp-2.0.0-rc3\n",
    "! wget https://github.com/ku-nlp/jumanpp/releases/download/v2.0.0-rc3/jumanpp-2.0.0-rc3.tar.xz\n",
    "! tar xvf jumanpp-2.0.0-rc3.tar.xz\n",
    "\n",
    "# jumanpp-2.0.0-rc3/libs/catch.hpp                  ~6609~  -> char FatalConditionHandler::altStackMem[32768] = {};\n",
    "# jumanpp-2.0.0-rc3/src/util/serialization_test.cc  ~top~   -> #include <limits>\n",
    "\n",
    "! mkdir jumanpp-2.0.0-rc3/bld\n",
    "! cd jumanpp-2.0.0-rc3/bld && cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX='where'\n",
    "! cd jumanpp-2.0.0-rc3/bld && make install -j 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART large v2.0 (3.7G)\n",
    "! wget http://lotus.kuee.kyoto-u.ac.jp/nl-resource/JapaneseBARTPretrainedModel/japanese_bart_large_2.0.tar.gz\n",
    "! tar -zxvf japanese_bart_large_2.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "# jumanpp = Juman(command = 'jumanpp-2.0.0-rc3/bld/where/bin/jumanpp')\n",
    "\n",
    "TRAIN_SRC = 'main_data/train_src.txt'\n",
    "TRAIN_TGT = 'main_data/train_tgt.txt'\n",
    "VALID_SRC = 'main_data/val_src.txt'\n",
    "VALID_TGT = 'main_data/val_tgt.txt'\n",
    "TEST_SRC = 'main_data/test_src.txt'\n",
    "TEST_TGT = 'main_data/test_tgt.txt'\n",
    "\n",
    "# 以下、モデルに応じて書き換え\n",
    "SENTENCEPIECE_MODEL = 'japanese_bart_large_2.0/sp.model'\n",
    "DICT = 'japanese_bart_large_2.0/dict.txt'\n",
    "DATASET_DIR = 'large2_dataset_dir'\n",
    "! mkdir $DATASET_DIR\n",
    "\n",
    "! cat $TRAIN_SRC | python3 jaBART_preprocess.py --bpe_model $SENTENCEPIECE_MODEL --bpe_dict $DICT > $DATASET_DIR/train.src-tgt.src\n",
    "! cat $TRAIN_TGT | python3 jaBART_preprocess.py  --bpe_model $SENTENCEPIECE_MODEL --bpe_dict $DICT > $DATASET_DIR/train.src-tgt.tgt\n",
    "! cat $VALID_SRC | python3 jaBART_preprocess.py --bpe_model $SENTENCEPIECE_MODEL --bpe_dict $DICT > $DATASET_DIR/valid.src-tgt.src\n",
    "! cat $VALID_TGT | python3 jaBART_preprocess.py --bpe_model $SENTENCEPIECE_MODEL --bpe_dict $DICT > $DATASET_DIR/valid.src-tgt.tgt\n",
    "! cat $TEST_SRC | python3 jaBART_preprocess.py --bpe_model $SENTENCEPIECE_MODEL --bpe_dict $DICT > $DATASET_DIR/test.src-tgt.src\n",
    "! cat $TEST_TGT | python3 jaBART_preprocess.py --bpe_model $SENTENCEPIECE_MODEL --bpe_dict $DICT > $DATASET_DIR/test.src-tgt.tgt\n",
    "! cp $DICT $DATASET_DIR/dict.src.txt\n",
    "! cp $DICT $DATASET_DIR/dict.tgt.txt\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairseq/fairseq/data/indexed_dataset.py \n",
    "# 98, 301 np.float -> float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune\n",
    "\n",
    "# 以下、モデルに応じて書き換え\n",
    "# DATASET_DIR = 'large2_dataset_dir'\n",
    "# BART = 'bart_large'\n",
    "# PRETRAINED_MODEL = 'japanese_bart_large_2.0/bart_model.pt'\n",
    "# SAVE_MODEL_DIR = 'large2_model_save'\n",
    "# ! mkdir large2_model_save\n",
    "# TENSORBOARD_DIR = 'large2_tensorboard'\n",
    "# ! mkdir large2_tensorboard\n",
    "\n",
    "# --restore-fileのcontentが必要みたい。\n",
    "! nohup fairseq-train large2_dataset_dir --arch bart_large --restore-file content/japanese_bart_large_2.0/bart_model.pt \\\n",
    "    --save-dir large2_model_save --tensorboard-logdir large2_tensorboard \\\n",
    "    --task translation_from_pretrained_bart --source-lang src --target-lang tgt \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.2 --dataset-impl raw \\\n",
    "    --optimizer adam --adam-eps 1e-06 --adam-betas '{0.9, 0.98}' --lr-scheduler polynomial_decay --lr 5e-05 --min-lr -1 \\\n",
    "    --warmup-updates 2500 --total-num-update 10000 --weight-decay 0.3 \\\n",
    "    --max-tokens 1024 --update-freq 2 --save-interval -1 --no-epoch-checkpoints --log-format simple --log-interval 2 \\\n",
    "    --reset-optimizer --reset-meters --reset-dataloader --reset-lr-scheduler  --save-interval-updates 5000 \\\n",
    "    --ddp-backend no_c10d --max-update 20000 \\\n",
    "    --encoder-normalize-before --decoder-normalize-before --langs ja --prepend-bos --patience 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-generate\n",
    "\n",
    "# DATASET_DIR = 'large2_dataset_dir'\n",
    "# RESULT = 'large2_ja2ja'\n",
    "# FINETUNED_MODEL = 'large2_model_save/checkpoint_best.pt'\n",
    "\n",
    "! fairseq-generate dataset_dir --path large2_model_save/checkpoint_best.pt --task translation_from_pretrained_bart \\\n",
    "    --dataset-impl raw --gen-subset test -s src -t tgt --max-sentences 64 --langs ja --prepend-bos > large2_ja2ja\n",
    "\n",
    "! grep ^H large2_ja2ja | LC_ALL=C sort -V | cut -f 3- | sed 's/<<unk>>/<unk>/g' | sed 's/▁//g' > large2_ja2ja.pred\n",
    "! grep ^S large2_ja2ja | LC_ALL=C sort -V | cut -f 2- | sed 's/<<unk>>/<unk>/g' | sed 's/▁//g' > large2_ja2ja.src\n",
    "! grep ^T large2_ja2ja | LC_ALL=C sort -V | cut -f 2- | sed 's/<<unk>>/<unk>/g' | sed 's/▁//g' > large2_ja2ja.tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-generate\n",
    "\n",
    "# DATASET_DIR = 'large2_dataset_dir'\n",
    "# RESULT = 'large2_ja2ja'\n",
    "# FINETUNED_MODEL = 'large2_model_save/checkpoint_best.pt'\n",
    "\n",
    "! fairseq-generate large2_dataset_dir --path large2_model_save/checkpoint_best.pt --task translation_from_pretrained_bart \\\n",
    "    --dataset-impl raw --gen-subset test -s src -t tgt --max-sentences 64 --langs ja --prepend-bos > large2_ja2ja\n",
    "\n",
    "! grep ^H large2_ja2ja | LC_ALL=C sort -V | cut -f 3- | sed 's/<<unk>>/<unk>/g' | sed 's/▁//g' > large2_ja2ja.pred\n",
    "! grep ^S large2_ja2ja | LC_ALL=C sort -V | cut -f 2- | sed 's/<<unk>>/<unk>/g' | sed 's/▁//g' > large2_ja2ja.src\n",
    "! grep ^T large2_ja2ja | LC_ALL=C sort -V | cut -f 2- | sed 's/<<unk>>/<unk>/g' | sed 's/▁//g' > large2_ja2ja.tgt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jafair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
